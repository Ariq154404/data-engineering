 My name is Ariq Rahman. I am going to write about my projects, work experience, education, certifications and contact details

Personal Projects:
Now  I am going to write about my personal projects that I did
MLops on AWS: In this personal project, I did MLops on AWS sagemaker in this project. I used the kaggle dataset which contained different kind of tweets . The tweets were mainly about climate change. The dataset also contained labels about the tweet indicating whether the tweet was neutral, positive about climate change or negative about climate change. I was trying to do a classification task to identify the kind of belief. Initially , I did preprocessing to split the data into training and testing set. Then I did label encoding to convert text labels into numbers. Then I did tokerization with Roberta model and stored the model into feature store. Then I trained Roberta model as supervised learning process. The training and testing data was stored in feature stores. I used the following python libraries: sklearn, multiprocessing, sklearn, transformer, numpy, pandas, tensorflow, argparse, boto3.
A/B testing on questionnaire dataset: In this personal project , I tried to do A/B testing on a dataset from kaggle that had column that indicated peoples response  to buying products based on  survey that was divided amongst control and exposed group. I found the p-value for the A/B testing . I used the following python libraries: numpy, pandas, scipy, seaborn, matplotlib, math, statsmodel
Climate Change tweet classification: In this personal project, I worked on the same climate change data. I trained three model which are bert, roberta, and google flan t5 and also did hyper parameter tuning and Exploratory data analysis. The experiment was tracked in the Wandbi Mlops platform. I used the following libraries in python: numpy, pandas , seaborn, matplotlib, spacy, nlkt, geopy, geopandas, tensorflow, huggingface, sklearn, keras tuner
Movie recommendation on AWS: In this personal project, I worked on  movie recommendation based on the IMDB dataset and recommended movies and also did an analysis of the imdb movie dataset . I used different kind on AWS technologies for this project. For movie recommendation, it was a semantic search. The recommendation worked in such a way that whenever a user imputed a description of the type of movie they wanted to see, the system would recommend k closest titiles.  I made a system design based on this using AWS services. All the movies descriptions were converted into tokens and stored as  embedding  in AWS opensearch embedding space. Then, using AWS lamba user description of the movie would be given as input to the function and K closest movie tittle would be retrieved based on cosine similarity of the movie description.
For the second part, I created a data analysis pipeline on AWS using AWS glue, AWS athena and AWS quicksight

Shell Script: In this personal project,I wrote shell script in unix to monitor all current user processes and if any processes had more that n descendants then output the process tree with p as the root on the standard output using pstree -p.
AWS orchestration of bit-coin forecast: In this personal project, I used AWS event bridge to orchestrate multiple notebooks that included model to forecast bit coin price . The notebooks were ran in AWS sagemaker. Each event like preprocessing, training, evaluation, e.t.c triggered the next stage of the pipeline. The notebooks were called from lambda functions. The libraries used in this project were boto3, sagemaker, pickle, matplotlib, pandas
Software Engineering project: In this personal project, II collaborated with five other classmates as part of  my software engineering course at university of windsor. The task was to locate different water fountains at university of windsor. I coded the backend for the project. I implemented different design patterns and created Flask based APIs for it . Then I deployed the API in heroku. I also wrote several unit test for different modules and also did code coverage. The libraries used in this project was Pandas, Flask, pytest. We used scrum methodology to develop the project
IEEE Cis  Fraud detection: In this personal project, I I took part in a binary classification project  on a structured dataset to detected fraud paper from non fraud paper. The dataset came from  Kaggle. I did Exploratory data Analysis. I did feature engineering like creating one hot encoder, removing null, balancing unbalanced data, imputing missing values. Then I did an ensemble of neural network and decision tree to do the classification. The   AUC public score was 89% on the Kaggle dataset.
Django project for Trekking: In this personal project, I collaborated in a django project where I build features for a trekking website. I build the login, logout page and forget password feature.
Tableau Data Analysis: In this project, I   utilized Tableau to design and develop an interactive dashboard displaying key insights and trends for a Addidas sales dataset, including geographical maps of store locations and percentage sales by state, a scatter plot showcasing sales and revenue for each store over two years, and a pie chart highlighting sales proportion by store. This dashboard provides actionable business intelligence to drive strategic decision-making and optimize sales performance.
Exploratory Data Analysis (EDA): I volunteered as a Data Analyst in Exploratory Data Analytics pipeline for the WiCys club in a project to classify different cybersecurity threats, where I analyzed and cleaned datasets by loading tabular data into Pandas , identifying missing data, plotting different statistical graph using Seaborn, Matplotlib and  extracting features based on statistical co relation that generated insights for other teams in pipeline to help them use minimum number of features that optimized classification model training time.
Work Experience
Now I am going to describe my work experience
Data Analytics Intern : I worked as Data Analytics intern at TD Bank of canada from April to August of 2023. I worked in the IT team. The job of our team was to prepare data analytics report for our stakeholders . However, due to some unconventional situation, we had to shift from  creating dashboard in servicenow to create dashboard in PoweBI. Here, I did two things. Our team did not have access to service now backend api, so I wrote a python selenium script to automate servicenow queries and  download the files as part of the ETL process.This streamlined the business process by trimming down time from manual tedious downloads. Secondly, I created dashboard in powerbi , using optimized powerquery for ETL  process. In the process, I also made data models in power bi , creating a start schema based on the service now data. The optimized power query for ETL decreased the time for query to run by 40%.
Artificial Intelligence Engineer: I worked as an AI Engineer at Intelligent Machines from Sept 2021 to April 2022. I worked in three project here mainly. 
First project was  a handwritten text recognition project for Unilever Bangladesh. Here I collaborated with 5 other team members. I performed data analysis, data visualization , feature engineering of over 700 GB of image data using Python Numpy , Pandas , Matplotlib in Azure Databricks cluster . I coded and trained deep learning classification model using  Tensorflow, Keras  in Azure Databricks.I deployed  deep learning model as docker Tensorflow Serving for batch analysis. I analysed over 8 hours of model training information in the Wandb MLops platform. I designed and coded algorithm in Python to optimize post-processing time by 500%. I communicated with client to understand requirement using Powerpoint and explained evaluation metrices.This model improved Unileverâ€™s customer conversion target by 260% in 2021

Second was a R&D project for Unilever bangladesh. I collaborated with 3 members for the project. This was a sound matching project . I designed and programmed an algorithm using Python and  Numpy data structure  on  4 GB  of sound data that optimized matching time by 300% over basic algorithm.
Education:
Now , I am going to write about my work experience . 
1. I completed Master of Applied Computing in ,Artificial Intelligence Stream from May 2022 to August 2023 from University of windsor , Windsor, Ontario
2.  I completed Bachelor of Science in Computer Science and Engineering from January 2016 to November 2019 at Islamic University of Technology, Dhaka, Bangladesh

Certifications:
I had two major certifications:
Tensorflow developer certification
Azure Associate Data Scientist certification Dp-100 certification

Contact details:
 Now I am going to write about my contact details
 My Mobile number: 4373660580
 My  Address: 3 Calumet Crescent, M1H 1W4
  



